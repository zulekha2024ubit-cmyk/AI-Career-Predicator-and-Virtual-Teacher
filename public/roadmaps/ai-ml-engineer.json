{
  "slug": "ai-ml-engineer",
  "title": "AI/ML Engineer",
  "summary": "Build production-grade intelligent systems with deep learning, LLMs, computer vision, NLP, and MLOps practices.",
  "timeline": "12-16 months",
  "difficulty": "Advanced",
  "steps": [
    {
      "id": "mathematics",
      "title": "Mathematics Foundation (6-8 weeks)",
      "description": "Master Linear Algebra, Calculus, Probability, Statistics, and Optimization - essential for understanding ML algorithms and deep learning.",
      "resources": [
        { "title": "Essence of Linear Algebra", "url": "https://www.youtube.com/embed/videoseries?si=m0jbE1TatCXA5AbV&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab", "type": "Video"},
        { "title": "Interactive Linear Algebra", "url": "https://textbooks.math.gatech.edu/ila/", "type": "Interactive" },
        { "title": "Essence of Calculus", "url": "https://www.youtube.com/embed/videoseries?si=Fgr9fcov_VrnqWhh&amp;list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr", "type": "Video"},
        { "title": "Chain Rule & Backpropagation", "url": "https://www.youtube.com/embed/tIeHLnjs5U8?si=1Xi5nXK7NpCIhCbD", "type": "Video" },
        { "title": "StatQuest - Statistics", "url": "https://www.youtube.com/embed/videoseries?si=WsdK8Q0tt8hWFNq9&amp;list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9", "type": "Video", "note": "BEST" },
        { "title": "Seeing Theory", "url": "https://seeing-theory.brown.edu/", "type": "Interactive", "note": "Visual Probability" },
        { "title": "Gradient Descent Visual", "url": "https://www.youtube.com/embed/IHZwWFHWa-w?si=GpT5sk1q0z54rimR", "type": "Video" },
        { "title": "Mathematics for ML", "url": "https://mml-book.github.io/", "type": "book", "note": "Free Book" }
      ],
      "quiz": [
        {"question": "What is a matrix?", "options": ["Rectangular array of numbers arranged in rows and columns", "Single number", "Graph", "Function"], "correctAnswer": 0, "explanation": "Matrices are fundamental in ML for representing data, transformations, and weights."},
        {"question": "What is the derivative used for in machine learning?", "options": ["Data storage", "Measuring rate of change to optimize loss functions", "Plotting graphs", "Sorting data"], "correctAnswer": 1, "explanation": "Derivatives compute gradients that guide optimization algorithms like gradient descent."},
        {"question": "What does the chain rule enable in neural networks?", "options": ["Data preprocessing", "Backpropagation for computing gradients through layers", "Forward pass", "Data augmentation"], "correctAnswer": 1, "explanation": "Chain rule allows computing gradients layer-by-layer during backpropagation."},
        {"question": "What is a probability distribution?", "options": ["Function describing likelihood of outcomes", "Database", "Neural network", "Optimization algorithm"], "correctAnswer": 0, "explanation": "Probability distributions model uncertainty in data and predictions."},
        {"question": "What is gradient descent?", "options": ["Data cleaning method", "Optimization algorithm minimizing loss by following gradients", "Feature engineering", "Model evaluation"], "correctAnswer": 1, "explanation": "Gradient descent iteratively adjusts parameters to minimize the loss function."},
        {"question": "What is the purpose of eigenvalues and eigenvectors?", "options": ["Dimensionality reduction techniques like PCA", "Data storage", "Model deployment", "Web scraping"], "correctAnswer": 0, "explanation": "Eigenvalues/eigenvectors identify principal components in PCA and other transformations."},
        {"question": "What is the dot product?", "options": ["Operation computing weighted sum, fundamental to neural networks", "Database query", "File format", "Sorting algorithm"], "correctAnswer": 0, "explanation": "Dot products compute neuron activations as weighted sums of inputs."},
        {"question": "What is a normal (Gaussian) distribution?", "options": ["Random process", "Bell-shaped probability distribution common in nature", "Neural network type", "Optimization method"], "correctAnswer": 1, "explanation": "Normal distribution models many natural phenomena and is used in statistical inference."},
        {"question": "What is the variance?", "options": ["Measure of data spread from the mean", "Average value", "Maximum value", "Data type"], "correctAnswer": 0, "explanation": "Variance quantifies how much data points deviate from the mean."},
        {"question": "What is a partial derivative?", "options": ["Derivative with respect to one variable while others are constant", "Complete derivative", "Integration", "Summation"], "correctAnswer": 0, "explanation": "Partial derivatives compute gradients for each parameter in multi-variable optimization."},
        {"question": "What is Bayes' Theorem?", "options": ["Rule for updating probabilities with new evidence", "Matrix operation", "Sorting algorithm", "Loss function"], "correctAnswer": 0, "explanation": "Bayes' Theorem: P(A|B) = P(B|A)P(A)/P(B), crucial in probabilistic ML."},
        {"question": "What is the transpose of a matrix?", "options": ["Inverse matrix", "Flipping rows and columns", "Diagonal elements", "Determinant"], "correctAnswer": 1, "explanation": "Transpose switches matrix rows with columns (A^T)."},
        {"question": "What does stochastic mean in Stochastic Gradient Descent?", "options": ["Random sampling of data batches", "Deterministic", "Sequential", "Parallel"], "correctAnswer": 0, "explanation": "Stochastic means using random subsets (batches) instead of entire dataset."},
        {"question": "What is a Hessian matrix?", "options": ["First derivatives", "Second-order partial derivatives for optimization", "Data matrix", "Weight matrix"], "correctAnswer": 1, "explanation": "Hessian captures curvature, used in second-order optimization methods."},
        {"question": "What is regularization in optimization?", "options": ["Adding penalty terms to prevent overfitting", "Removing data", "Speeding up training", "Data normalization"], "correctAnswer": 0, "explanation": "Regularization (L1/L2) constrains model complexity."},
        {"question": "What is the Central Limit Theorem?", "options": ["Sample means approach normal distribution with large n", "All data is normal", "Medians are means", "Variance is constant"], "correctAnswer": 0, "explanation": "CLT underpins statistical inference and confidence intervals."},
        {"question": "What is covariance?", "options": ["Measure of joint variability between two variables", "Individual variance", "Mean difference", "Standard deviation"], "correctAnswer": 0, "explanation": "Covariance indicates how two variables change together."},
        {"question": "What is the identity matrix?", "options": ["All ones", "Ones on diagonal, zeros elsewhere", "All zeros", "Random values"], "correctAnswer": 1, "explanation": "Identity matrix I leaves vectors unchanged: Iv = v."},
        {"question": "What is convexity in optimization?", "options": ["Multiple local minima", "Single global minimum property", "No minima", "Infinite minima"], "correctAnswer": 1, "explanation": "Convex functions guarantee gradient descent finds global optimum."},
        {"question": "What is the Jacobian matrix?", "options": ["Matrix of first-order partial derivatives", "Second derivatives", "Data matrix", "Correlation matrix"], "correctAnswer": 0, "explanation": "Jacobian represents gradients of vector-valued functions."}
      ]
    },
    {
      "id": "python-programming",
      "title": "Python Programming (4-6 weeks)",
      "description": "Master Python fundamentals, OOP, NumPy, and Pandas for data manipulation and algorithm implementation.",
      "resources": [
        { "title": "Python Full Course", "url": "https://www.youtube.com/embed/rfscVS0vtbw?si=9yQIt3wntC907wFp", "type": "Video", "duration": "4.5 hours" },
        { "title": "Python Official Tutorial", "url": "https://docs.python.org/3/tutorial/", "type": "Article", "note": "Best Documentation" },
        { "title": "Python OOP Tutorial", "url": "https://www.youtube.com/embed/videoseries?si=l0xNFJkF8j3SQ7DD&amp;list=PL-osiE80TeTsqhIuOqKhwlXsIBIdSeYtc", "type": "Video", "note": "Corey Schafer" },
        { "title": "NumPy Complete Course", "url": "https://www.youtube.com/embed/QUT1VHiLmmI?si=5yQOWyhKDKBuaZvB", "type": "Video", "duration": "1 hour" },
        { "title": "NumPy Official Quickstart", "url": "https://numpy.org/doc/stable/user/quickstart.html", "type": "Article" },
        { "title": "Pandas Full Course", "url": "https://www.youtube.com/embed/vmEHCJofslg?si=q6tOqFhsaItUM_o5", "type": "Video", "duration": "1 hours", "note": "Keith Galli - BEST" },
        { "title": "Pandas Official Tutorial", "url": "https://pandas.pydata.org/docs/getting_started/intro_tutorials/", "type": "Article" },
        { "title": "10 Minutes to Pandas", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "type": "Article" },
        { "title": "Python Data Science Handbook", "url": "https://jakevdp.github.io/PythonDataScienceHandbook/", "type": "Book", "note": "Free" }
      ],
      "quiz": [
        {"question": "What is a Python list comprehension?", "options": ["Concise way to create lists using iteration", "Database query", "File format", "Class definition"], "correctAnswer": 0, "explanation": "List comprehensions provide compact syntax: [x**2 for x in range(10)]."},
        {"question": "What is the difference between a list and a tuple?", "options": ["Lists are mutable, tuples are immutable", "No difference", "Tuples are faster", "Lists use more memory"], "correctAnswer": 0, "explanation": "Lists can be modified after creation; tuples cannot be changed."},
        {"question": "What is a NumPy array?", "options": ["Database", "Multi-dimensional array for efficient numerical computation", "Web framework", "File format"], "correctAnswer": 1, "explanation": "NumPy arrays enable vectorized operations, much faster than Python lists."},
        {"question": "What does Pandas DataFrame represent?", "options": ["Neural network", "2D labeled data structure similar to a table", "Image format", "Web server"], "correctAnswer": 1, "explanation": "DataFrames organize data in rows and columns with labeled axes."},
        {"question": "What is broadcasting in NumPy?", "options": ["Automatic expansion of arrays for element-wise operations", "Network protocol", "File transfer", "Data visualization"], "correctAnswer": 0, "explanation": "Broadcasting allows operations on arrays of different shapes without explicit replication."},
        {"question": "What is the purpose of 'self' in Python classes?", "options": ["References the instance of the class", "Global variable", "Function name", "Module import"], "correctAnswer": 0, "explanation": "'self' is the first parameter in instance methods, referring to the object itself."},
        {"question": "What does the Pandas 'groupby()' function do?", "options": ["Deletes rows", "Groups data by values for aggregation operations", "Sorts data", "Merges DataFrames"], "correctAnswer": 1, "explanation": "groupby() enables split-apply-combine operations for data aggregation."},
        {"question": "What is a Python decorator?", "options": ["Function that modifies behavior of another function", "Data type", "Loop structure", "Import statement"], "correctAnswer": 0, "explanation": "Decorators wrap functions to add functionality, using @ syntax."},
        {"question": "What is vectorization?", "options": ["Image processing", "Performing operations on entire arrays without explicit loops", "Data cleaning", "Model training"], "correctAnswer": 1, "explanation": "Vectorization leverages optimized C/Fortran code for much faster computation."},
        {"question": "What does 'np.reshape()' do?", "options": ["Changes array dimensions without changing data", "Deletes elements", "Sorts array", "Creates new array"], "correctAnswer": 0, "explanation": "reshape() reorganizes array elements into a new shape while preserving data."},
        {"question": "What is a Python generator?", "options": ["Function using 'yield' to produce values lazily", "List", "Dictionary", "Class"], "correctAnswer": 0, "explanation": "Generators save memory by yielding values one at a time."},
        {"question": "What does lambda function provide?", "options": ["Named function", "Class definition", "Anonymous single-expression function", "Import statement"], "correctAnswer": 2, "explanation": "Lambda creates inline functions: lambda x: x**2."},
        {"question": "Difference between shallow and deep copy?", "options": ["Deep copies nested objects recursively; shallow doesn't", "No difference", "Shallow is faster", "Deep uses less memory"], "correctAnswer": 0, "explanation": "Deep copy duplicates all nested structures; shallow shares references."},
        {"question": "What is *args in functions?", "options": ["Variable number of positional arguments", "Keyword arguments", "Class method", "Decorator"], "correctAnswer": 0, "explanation": "*args collects arbitrary positional arguments into tuple."},
        {"question": "What does enumerate() provide?", "options": ["Only indices", "Index and value pairs during iteration", "Values only", "Sorted list"], "correctAnswer": 1, "explanation": "enumerate() yields (index, value) tuples: enumerate(['a','b'])."},
        {"question": "Pandas loc vs iloc?", "options": ["loc uses labels; iloc uses integer positions", "Same thing", "iloc is deprecated", "loc is faster"], "correctAnswer": 0, "explanation": "loc selects by row/column labels; iloc by integer index."},
        {"question": "What is __init__ method?", "options": ["Destructor", "Constructor initializing instance", "Static method", "Class variable"], "correctAnswer": 1, "explanation": "__init__ is called when creating new class instance."},
        {"question": "NumPy np.dot() performs?", "options": ["Element-wise multiplication", "Matrix/dot product", "Addition", "Transpose"], "correctAnswer": 1, "explanation": "dot() computes matrix multiplication or dot product."},
        {"question": "What is try/except used for?", "options": ["Exception handling", "Loops", "Conditionals", "Imports"], "correctAnswer": 0, "explanation": "try/except blocks handle runtime errors gracefully."},
        {"question": "Pandas merge() does?", "options": ["Joins DataFrames on common columns like SQL", "Deletes rows", "Sorts data", "Filters data"], "correctAnswer": 0, "explanation": "merge() combines DataFrames using inner/outer/left/right joins."}
      ]
    },
    {
      "id": "data-engineering",
      "title": "Data Engineering Basics (2-3 weeks)",
      "description": "Learn data pipelines, SQL/NoSQL databases, ETL processes, and big data basics with Spark.",
      "resources": [
        { "title": "Data Engineering", "url": "https://www.youtube.com/embed/PHsC_t0j1dU?si=meD5JROeTo3q0pEv", "type": "Video" },
        { "title": "PySpark Tutorial", "url": "https://www.youtube.com/embed/_C8kWso4ne4?si=7Hl77GPvMzBQBCNi", "type": "Video" }
      ],
      "quiz": [
        {"question": "What is ETL?", "options": ["Extract, Transform, Load - process for moving data", "Error Testing Language", "Export Table Logic", "Encryption Transfer Layer"], "correctAnswer": 0, "explanation": "ETL extracts data from sources, transforms it, and loads into target systems."},
        {"question": "What is a data pipeline?", "options": ["Automated workflow for moving and transforming data", "Database", "Neural network", "Web server"], "correctAnswer": 0, "explanation": "Pipelines automate data ingestion, processing, and delivery to destinations."},
        {"question": "What is Apache Spark?", "options": ["Web framework", "Distributed computing system for big data processing", "Database", "Operating system"], "correctAnswer": 1, "explanation": "Spark processes large datasets in parallel across clusters with in-memory computation."},
        {"question": "What is the difference between SQL and NoSQL databases?", "options": ["SQL is structured/relational, NoSQL is flexible/non-relational", "No difference", "NoSQL is faster", "SQL is newer"], "correctAnswer": 0, "explanation": "SQL databases use tables and schemas; NoSQL uses documents, key-value, or graphs."},
        {"question": "What is data partitioning?", "options": ["Deleting data", "Dividing data into smaller chunks for parallel processing", "Encrypting data", "Backing up data"], "correctAnswer": 1, "explanation": "Partitioning improves performance by distributing data across nodes."},
        {"question": "What is PySpark?", "options": ["Python API for Apache Spark", "Database", "Web framework", "Visualization tool"], "correctAnswer": 0, "explanation": "PySpark enables distributed data processing using Python with Spark."},
        {"question": "What is a data lake?", "options": ["Centralized repository storing raw, unstructured data", "Relational database", "Spreadsheet", "Cache memory"], "correctAnswer": 0, "explanation": "Data lakes store diverse data formats at scale for future processing."},
        {"question": "What is batch processing?", "options": ["Processing data in large groups at scheduled intervals", "Real-time processing", "Manual processing", "Deleting data"], "correctAnswer": 0, "explanation": "Batch processing handles large volumes periodically, optimizing resource use."},
        {"question": "What is stream processing?", "options": ["Processing data in real-time as it arrives", "Batch processing", "Data storage", "Backup process"], "correctAnswer": 0, "explanation": "Stream processing analyzes continuous data flows for immediate insights."},
        {"question": "What is data schema?", "options": ["Structure defining data organization and types", "Database name", "Query language", "Storage format"], "correctAnswer": 0, "explanation": "Schemas define table structures, columns, data types, and relationships."},
        {"question": "What is Apache Kafka?", "options": ["Distributed streaming platform", "Database", "Web server", "Machine learning tool"], "correctAnswer": 0, "explanation": "Kafka handles real-time data streams and message queuing."},
        {"question": "What is data warehouse?", "options": ["Centralized store for structured, processed data", "Raw data storage", "Cache", "Log file"], "correctAnswer": 0, "explanation": "Warehouses optimize analytical queries on structured data."},
        {"question": "What is data lineage?", "options": ["Tracking data flow from origin to destination", "Data size", "Data speed", "Data color"], "correctAnswer": 0, "explanation": "Lineage maps data transformation and dependencies."},
        {"question": "What is Airflow?", "options": ["Workflow orchestration platform", "Database", "Cloud provider", "Visualization tool"], "correctAnswer": 0, "explanation": "Airflow schedules and monitors data pipelines as DAGs."},
        {"question": "What is sharding?", "options": ["Horizontal partitioning across databases", "Vertical partitioning", "Backup", "Encryption"], "correctAnswer": 0, "explanation": "Sharding distributes rows across multiple database instances."},
        {"question": "What is idempotency in pipelines?", "options": ["Multiple executions produce same result", "Speed optimization", "Data compression", "Error handling"], "correctAnswer": 0, "explanation": "Idempotent operations safely retry without side effects."},
        {"question": "What is OLTP vs OLAP?", "options": ["OLTP: transactional; OLAP: analytical", "Same thing", "OLAP is newer", "OLTP is faster"], "correctAnswer": 0, "explanation": "OLTP handles transactions; OLAP optimizes complex queries."},
        {"question": "What is data ingestion?", "options": ["Process of importing data into system", "Data deletion", "Data export", "Data visualization"], "correctAnswer": 0, "explanation": "Ingestion collects data from various sources into processing system."},
        {"question": "What is eventual consistency?", "options": ["Data becomes consistent over time in distributed systems", "Immediate consistency", "No consistency", "Perfect consistency"], "correctAnswer": 0, "explanation": "Eventual consistency trades immediate accuracy for availability."},
        {"question": "What is a message queue?", "options": ["System for asynchronous communication between services", "Database", "File storage", "Web server"], "correctAnswer": 0, "explanation": "Message queues decouple services for reliable data exchange."}
      ]
    },
    {
      "id": "classical-ml",
      "title": "Classical Machine Learning (6-8 weeks)",
      "description": "Master supervised/unsupervised learning with scikit-learn, and algorithms like Random Forests, XGBoost, SVM, K-Means, PCA.",
      "resources": [
        { "title": "Conceptual Foundations: StatQuest Machine Learning", "url": "https://www.youtube.com/embed/videoseries?si=qnUR_i0VVGVnAI8j&amp;list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF" , "type": "Video"},
        { "title": "ML Full Course", "url": "https://www.youtube.com/embed/videoseries?si=2RtjCyWRjIjqKr7C&amp;list=PLZoTAELRMXVPBTrWtJkn3wWQxZkmTXGwe", "type": "video"}
      ],
      "quiz": [
        {"question": "What is supervised learning?", "options": ["Learning from labeled data with input-output pairs", "Learning without labels", "Reinforcement learning", "Unsupervised clustering"], "correctAnswer": 0, "explanation": "Supervised learning trains models on labeled examples to predict outputs for new inputs."},
        {"question": "What is unsupervised learning?", "options": ["Learning from labeled data", "Finding patterns in unlabeled data", "Reinforcement learning", "Supervised regression"], "correctAnswer": 1, "explanation": "Unsupervised learning discovers hidden structures without labeled outputs."},
        {"question": "What is overfitting?", "options": ["Model performs well on training but poorly on new data", "Model performs poorly on all data", "Perfect model", "Underfitting"], "correctAnswer": 0, "explanation": "Overfitting occurs when a model memorizes training data instead of learning patterns."},
        {"question": "What is a Random Forest?", "options": ["Database", "Ensemble of decision trees for classification/regression", "Neural network", "Clustering algorithm"], "correctAnswer": 1, "explanation": "Random Forest combines multiple decision trees to improve accuracy and reduce overfitting."},
        {"question": "What is cross-validation?", "options": ["Technique to assess model performance on multiple data splits", "Data cleaning", "Feature engineering", "Model deployment"], "correctAnswer": 0, "explanation": "Cross-validation splits data into folds to evaluate model generalization."},
        {"question": "What is K-Means clustering?", "options": ["Supervised learning", "Unsupervised algorithm grouping data into K clusters", "Regression method", "Neural network"], "correctAnswer": 1, "explanation": "K-Means partitions data into K clusters based on feature similarity."},
        {"question": "What is PCA (Principal Component Analysis)?", "options": ["Dimensionality reduction technique preserving variance", "Classification algorithm", "Clustering method", "Regression technique"], "correctAnswer": 0, "explanation": "PCA transforms data to orthogonal components, reducing dimensions while retaining information."},
        {"question": "What is XGBoost?", "options": ["Extreme Gradient Boosting - powerful ensemble method", "Database", "Web framework", "Neural network type"], "correctAnswer": 0, "explanation": "XGBoost uses gradient boosting to combine weak learners into strong predictors."},
        {"question": "What is a confusion matrix?", "options": ["Table showing true positives, false positives, true negatives, false negatives", "Neural network layer", "Data structure", "Optimization method"], "correctAnswer": 0, "explanation": "Confusion matrix visualizes classification performance across all classes."},
        {"question": "What is the bias-variance tradeoff?", "options": ["Balance between model simplicity and complexity", "Data cleaning method", "Feature selection", "Training speed"], "correctAnswer": 0, "explanation": "High bias = underfitting; high variance = overfitting; optimal models balance both."},
        {"question": "What is regularization?", "options": ["Adding penalty terms to prevent overfitting", "Data normalization", "Feature selection", "Data augmentation"], "correctAnswer": 0, "explanation": "Regularization (L1/L2) constrains model complexity during training."},
        {"question": "What is precision in classification?", "options": ["TP / (TP + FP) - accuracy of positive predictions", "TP / (TP + FN)", "Accuracy", "F1-score"], "correctAnswer": 0, "explanation": "Precision measures correctness of positive predictions."},
        {"question": "What is recall?", "options": ["TP / (TP + FN) - coverage of actual positives", "TP / (TP + FP)", "Accuracy", "Specificity"], "correctAnswer": 0, "explanation": "Recall measures how many actual positives were identified."},
        {"question": "What is F1-score?", "options": ["Harmonic mean of precision and recall", "Arithmetic mean", "Accuracy", "AUC"], "correctAnswer": 0, "explanation": "F1-score balances precision and recall: 2*(P*R)/(P+R)."},
        {"question": "What is ensemble learning?", "options": ["Combining multiple models for better predictions", "Single model", "Data preprocessing", "Feature engineering"], "correctAnswer": 0, "explanation": "Ensemble methods (bagging, boosting, stacking) reduce variance and bias."},
        {"question": "What is bagging?", "options": ["Bootstrap Aggregating - training models on random samples", "Boosting variant", "Single model", "Data cleaning"], "correctAnswer": 0, "explanation": "Bagging reduces variance by averaging predictions from multiple models."},
        {"question": "What is boosting?", "options": ["Sequential ensemble correcting previous model errors", "Parallel ensemble", "Single model", "Data augmentation"], "correctAnswer": 0, "explanation": "Boosting (AdaBoost, GradientBoost) focuses on hard examples iteratively."},
        {"question": "What is support vector in SVM?", "options": ["Data points closest to decision boundary", "All data points", "Outliers", "Centers"], "correctAnswer": 0, "explanation": "Support vectors define the maximum margin hyperplane."},
        {"question": "What is k in k-NN?", "options": ["Number of nearest neighbors to consider", "Number of clusters", "Number of features", "Number of iterations"], "correctAnswer": 0, "explanation": "k-NN classifies based on majority vote of k closest points."},
        {"question": "What is the elbow method?", "options": ["Finding optimal K in K-Means clustering", "Feature selection", "Hyperparameter tuning", "Data splitting"], "correctAnswer": 0, "explanation": "Elbow method identifies K where inertia reduction slows significantly."}
      ]
    },
    {
      "id": "feature-engineering",
      "title": "Feature Engineering & Selection (2-3 weeks)",
      "description": "Learn feature scaling, encoding, creation, dimensionality reduction, and handling imbalanced data.",
      "resources": [
        { "title": "Feature Engineering for ML", "url": "https://www.youtube.com/embed/videoseries?si=WqiS2VLE_oEFt4FB&amp;list=PLKnIA16_RmvYXWH_E6PuVLLHHTWXwwDN7", "type": "Video" }
      ],
      "quiz": [
        {"question": "What is feature scaling?", "options": ["Normalizing features to similar ranges for better model performance", "Creating new features", "Removing features", "Data augmentation"], "correctAnswer": 0, "explanation": "Scaling prevents features with large ranges from dominating distance-based algorithms."},
        {"question": "What is one-hot encoding?", "options": ["Converting categorical variables into binary vectors", "Scaling numbers", "Data cleaning", "Model training"], "correctAnswer": 0, "explanation": "One-hot encoding represents categories as binary columns, enabling ML algorithms to process them."},
        {"question": "What is feature selection?", "options": ["Identifying and keeping most relevant features", "Creating features", "Scaling features", "Data visualization"], "correctAnswer": 0, "explanation": "Feature selection reduces dimensionality by removing irrelevant or redundant features."},
        {"question": "What is the difference between normalization and standardization?", "options": ["Normalization scales to [0,1], standardization to mean=0 std=1", "No difference", "Both are the same", "Normalization is better"], "correctAnswer": 0, "explanation": "Normalization: (x-min)/(max-min); Standardization: (x-mean)/std."},
        {"question": "What is feature engineering?", "options": ["Creating new features from existing data to improve models", "Model training", "Data collection", "Model deployment"], "correctAnswer": 0, "explanation": "Feature engineering transforms raw data into informative features that capture patterns."},
        {"question": "What is label encoding?", "options": ["Database operation", "Assigning integers to categorical values", "Feature scaling", "Model evaluation"], "correctAnswer": 1, "explanation": "Label encoding converts categories to numbers (e.g., 'red'=0, 'blue'=1)."},
        {"question": "What is handling imbalanced data?", "options": ["Addressing unequal class distribution in datasets", "Feature scaling", "Data visualization", "Model deployment"], "correctAnswer": 0, "explanation": "Techniques include oversampling, undersampling, SMOTE, and class weights."},
        {"question": "What is polynomial feature creation?", "options": ["Generating interaction and higher-degree terms", "Deleting features", "Scaling features", "Data cleaning"], "correctAnswer": 0, "explanation": "Polynomial features capture non-linear relationships (e.g., xÂ² or x*y)."},
        {"question": "What is the curse of dimensionality?", "options": ["Performance degradation as feature count increases", "Too few features", "Fast training", "Better accuracy"], "correctAnswer": 0, "explanation": "High dimensions cause data sparsity, making models harder to train and generalize."},
        {"question": "What is target encoding?", "options": ["Encoding categories based on target variable statistics", "Label encoding", "One-hot encoding", "Scaling"], "correctAnswer": 0, "explanation": "Target encoding replaces categories with mean target value for that category."},
        {"question": "What is SMOTE?", "options": ["Synthetic Minority Over-sampling Technique", "Database", "Cloud service", "Optimizer"], "correctAnswer": 0, "explanation": "SMOTE generates synthetic samples for minority class to balance dataset."},
        {"question": "What is binning?", "options": ["Grouping continuous values into discrete bins", "Encoding categories", "Scaling", "Normalization"], "correctAnswer": 0, "explanation": "Binning discretizes continuous features (e.g., age groups)."},
        {"question": "What is log transformation?", "options": ["Reduces skewness in distribution", "Scales to [0,1]", "Removes outliers", "Encodes categories"], "correctAnswer": 0, "explanation": "Log transform makes right-skewed data more symmetric."},
        {"question": "What is feature hashing?", "options": ["Maps features to fixed-size vector via hash function", "Encryption", "Deletion", "Scaling"], "correctAnswer": 0, "explanation": "Hashing handles high-cardinality categories efficiently."},
        {"question": "What is variance threshold?", "options": ["Removes low or zero variance features", "Scales features", "Encodes categories", "Creates features"], "correctAnswer": 0, "explanation": "Low variance features provide little discriminative information."},
        {"question": "What is interaction feature?", "options": ["Combination of multiple features capturing joint effects", "Single feature", "Target variable", "Label"], "correctAnswer": 0, "explanation": "Interaction terms (x1*x2) model how features influence each other."},
        {"question": "What is outlier detection?", "options": ["Identifying data points far from normal range", "Feature creation", "Scaling", "Encoding"], "correctAnswer": 0, "explanation": "Methods include IQR, z-score, and isolation forest."},
        {"question": "What is missing value imputation?", "options": ["Filling missing values with estimates", "Deleting all data", "Ignoring missing values", "Creating features"], "correctAnswer": 0, "explanation": "Imputation uses mean, median, mode, or model predictions."},
        {"question": "What is ordinal encoding?", "options": ["Encoding ordered categories with integers preserving order", "One-hot encoding", "Random encoding", "Target encoding"], "correctAnswer": 0, "explanation": "Ordinal encoding maintains category order (low=0, medium=1, high=2)."},
        {"question": "What is RobustScaler?", "options": ["Scales using median and IQR, robust to outliers", "Standard scaler", "Min-max scaler", "Normalizer"], "correctAnswer": 0, "explanation": "RobustScaler: (x - median) / IQR, less sensitive to outliers."}
      ]
    },
    {
      "id": "neural-networks",
      "title": "Neural Networks Fundamentals (4-6 weeks)",
      "description": "Understand backpropagation, activation functions, optimizers with 3Blue1Brown's visual explanations and Andrew Ng's Deep Learning course.",
      "resources": [
        { "title": "Neural Networks by 3Blue1Brown", "url": "https://www.youtube.com/embed/videoseries?si=K1jKFD70ZbRaCMn0&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi" , "type": "Video", "note": "BEST Visual Explanation" },
        { "title": "Neural Networks from Scratch", "url": "https://www.youtube.com/embed/videoseries?si=0-rBQ2SDayp-nNHw&amp;list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3", "type": "Video"},
        { "title": "Activation Functions Explained", "url": "https://www.youtube.com/embed/m0pIlLfpXWE?si=0ZwbKW-P929aTsMR", "type": "video" },
        { "title": "Optimizers Explained", "url": "https://www.youtube.com/embed/mdKjMPmcWjY?si=WMuxBcvOdAljkJ7f", "type": "Video"},
        { "title": "Neural Networks & Deep Learning", "url": "http://neuralnetworksanddeeplearning.com/", "type": "Book", "note": "Free Book" }
      ],
      "quiz": [
        {"question": "What is a neural network?", "options": ["Computational model inspired by biological neurons", "Database", "Operating system", "Web server"], "correctAnswer": 0, "explanation": "Neural networks consist of interconnected layers that learn patterns from data."},
        {"question": "What is backpropagation?", "options": ["Forward pass", "Algorithm for computing gradients to update weights", "Data preprocessing", "Model deployment"], "correctAnswer": 1, "explanation": "Backpropagation calculates gradients layer-by-layer using the chain rule."},
        {"question": "What is an activation function?", "options": ["Non-linear function introducing non-linearity to networks", "Loss function", "Optimizer", "Data loader"], "correctAnswer": 0, "explanation": "Activation functions (ReLU, sigmoid, tanh) enable networks to learn complex patterns."},
        {"question": "What is ReLU?", "options": ["Database", "Rectified Linear Unit - activation function f(x)=max(0,x)", "Optimizer", "Loss function"], "correctAnswer": 1, "explanation": "ReLU is popular for its simplicity and effectiveness in deep networks."},
        {"question": "What is the purpose of a loss function?", "options": ["Measures model prediction error to guide optimization", "Activates neurons", "Loads data", "Deploys model"], "correctAnswer": 0, "explanation": "Loss functions quantify the difference between predictions and actual values."},
        {"question": "What is gradient descent with momentum?", "options": ["Optimization method using past gradients to accelerate convergence", "Data cleaning", "Feature engineering", "Model evaluation"], "correctAnswer": 0, "explanation": "Momentum helps escape local minima and speeds up training."},
        {"question": "What is the vanishing gradient problem?", "options": ["Gradients become too small in deep networks, slowing learning", "Gradients too large", "Fast training", "Overfitting"], "correctAnswer": 0, "explanation": "Vanishing gradients occur in deep networks with sigmoid/tanh, hindering weight updates."},
        {"question": "What is Adam optimizer?", "options": ["Adaptive moment estimation - combines momentum and RMSprop", "Loss function", "Activation function", "Data loader"], "correctAnswer": 0, "explanation": "Adam adapts learning rates for each parameter, widely used for its efficiency."},
        {"question": "What is a hidden layer?", "options": ["Layer between input and output that learns representations", "Input layer", "Output layer", "Loss function"], "correctAnswer": 0, "explanation": "Hidden layers extract hierarchical features from input data."},
        {"question": "What is dropout?", "options": ["Regularization technique randomly dropping neurons during training", "Loss function", "Optimizer", "Data augmentation"], "correctAnswer": 0, "explanation": "Dropout prevents overfitting by forcing the network to learn robust features."},
        {"question": "What is batch normalization?", "options": ["Normalizes layer inputs for stable training", "Data normalization", "Loss function", "Optimizer"], "correctAnswer": 0, "explanation": "Batch norm standardizes activations reducing internal covariate shift."},
        {"question": "What is learning rate?", "options": ["Step size controlling weight updates", "Data loading speed", "Batch size", "Number of epochs"], "correctAnswer": 0, "explanation": "Learning rate determines how much weights change per iteration."},
        {"question": "What is epoch?", "options": ["One complete pass through training dataset", "Single batch", "Learning rate", "Weight update"], "correctAnswer": 0, "explanation": "Training typically requires multiple epochs to converge."},
        {"question": "What is batch size?", "options": ["Number of samples processed before updating weights", "Epochs", "Learning rate", "Number of layers"], "correctAnswer": 0, "explanation": "Smaller batches provide noisier but more frequent updates."},
        {"question": "What is early stopping?", "options": ["Halting training when validation loss stops improving", "Fast training", "Data augmentation", "Feature engineering"], "correctAnswer": 0, "explanation": "Early stopping prevents overfitting by monitoring validation metrics."},
        {"question": "What is weight initialization?", "options": ["Setting initial weights before training", "Final weights", "Data loading", "Loss calculation"], "correctAnswer": 0, "explanation": "Proper initialization (Xavier, He) prevents vanishing/exploding gradients."},
        {"question": "What is softmax?", "options": ["Activation converting logits to probabilities", "Loss function", "Optimizer", "Hidden activation"], "correctAnswer": 0, "explanation": "Softmax outputs sum to 1, used in multi-class classification."},
        {"question": "What is cross-entropy loss?", "options": ["Loss for classification measuring predicted vs true distribution", "Regression loss", "Optimizer", "Activation"], "correctAnswer": 0, "explanation": "Cross-entropy penalizes incorrect class probability assignments."},
        {"question": "What is gradient clipping?", "options": ["Limiting gradient magnitude to prevent exploding gradients", "Removing gradients", "Increasing gradients", "Data clipping"], "correctAnswer": 0, "explanation": "Clipping caps gradients preventing numerical instability."},
        {"question": "What is a perceptron?", "options": ["Simplest neural network with single layer", "Multi-layer network", "CNN", "RNN"], "correctAnswer": 0, "explanation": "Perceptron is linear classifier, foundation of neural networks."}
      ]
    },
    {
      "id": "deep-learning-frameworks",
      "title": "Deep Learning Frameworks: PyTorch & TensorFlow (4-6 weeks)",
      "description": "Master PyTorch (primary) or TensorFlow for building, training, and deploying neural networks. Learn tensors, autograd, and transfer learning.",
      "resources": [
        { "title": "PyTorch 60 Minute Blitz", "url": "https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html", "type": "Article" },
        { "title": "PyTorch Full Course", "url": "https://www.youtube.com/embed/V_xro1bcAuA?si=r-9cSLJy1TUGZcHk", "type": "Video", "duration": "10 hours", "note": "freeCodeCamp - BEST" },
        { "title": "Fast.ai - Practical DL", "url": "https://course.fast.ai/", "type": "course", "note": "Top-Down Approach" },
        { "title": "PyTorch Tensors Tutorial", "url": "https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html", "type": "Article" },
        { "title": "Autograd Explained", "url": "https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html", "type": "Article" },
        { "title": "Building Neural Networks", "url": "https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html", "type": "Article" },
        { "title": "Training Loop Tutorial", "url": "https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html", "type": "Article" },
        { "title": "Transfer Learning Guide", "url": "https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html", "type": "Article" },
        { "title": "PyTorch Lightning in 15min", "url": "https://www.youtube.com/embed/Hgg8Xy6IRig?si=snoXBA3j84f6bx0N", "type": "Video" },
        { "title": "TensorFlow Full Course", "url": "https://www.youtube.com/embed/tPYj3fFJGjk?si=rw8xcqf7p4PR_wbw", "type": "Video", "duration": "7 hours" },
        { "title": "Keras Documentation", "url": "https://keras.io/guides/", "type": "Article", "note": "For Beginners" }
      ],
      "quiz": [
        {"question": "What is PyTorch?", "options": ["Open-source deep learning framework developed by Meta", "Database", "Web framework", "Operating system"], "correctAnswer": 0, "explanation": "PyTorch provides dynamic computation graphs and is widely used in research."},
        {"question": "What is a tensor?", "options": ["Multi-dimensional array, fundamental data structure in deep learning", "Database table", "Function", "File format"], "correctAnswer": 0, "explanation": "Tensors generalize scalars, vectors, and matrices to arbitrary dimensions."},
        {"question": "What is autograd in PyTorch?", "options": ["Automatic differentiation for computing gradients", "Data loader", "Model architecture", "Loss function"], "correctAnswer": 0, "explanation": "Autograd automatically tracks operations and computes gradients during backpropagation."},
        {"question": "What is transfer learning?", "options": ["Data transfer", "Using pre-trained models for new tasks", "Model deployment", "Data augmentation"], "correctAnswer": 1, "explanation": "Transfer learning leverages knowledge from models trained on large datasets."},
        {"question": "What is TensorFlow?", "options": ["Deep learning framework developed by Google", "Database", "Cloud platform", "Programming language"], "correctAnswer": 0, "explanation": "TensorFlow is widely used for production ML with static computation graphs."},
        {"question": "What is Keras?", "options": ["Database", "High-level neural network API running on TensorFlow", "Web server", "Cloud service"], "correctAnswer": 1, "explanation": "Keras simplifies building neural networks with user-friendly APIs."},
        {"question": "What is the difference between PyTorch and TensorFlow?", "options": ["PyTorch uses dynamic graphs, TensorFlow uses static (historically)", "No difference", "PyTorch is faster", "TensorFlow is newer"], "correctAnswer": 0, "explanation": "PyTorch's dynamic graphs are more flexible; TensorFlow 2.x added eager execution."},
        {"question": "What is a DataLoader in PyTorch?", "options": ["Iterator providing batches of data during training", "Model architecture", "Loss function", "Optimizer"], "correctAnswer": 0, "explanation": "DataLoader handles batching, shuffling, and parallel data loading."},
        {"question": "What is model.eval() in PyTorch?", "options": ["Deletes model", "Sets model to evaluation mode, disabling dropout/batchnorm training behavior", "Trains model", "Saves model"], "correctAnswer": 1, "explanation": "eval() mode ensures consistent inference behavior without training-specific operations."},
        {"question": "What is PyTorch Lightning?", "options": ["Lightweight wrapper organizing PyTorch code for scalability", "Loss function", "Activation function", "Data format"], "correctAnswer": 0, "explanation": "Lightning removes boilerplate, enabling cleaner, more modular code."},
        {"question": "What is torch.nn.Module?", "options": ["Base class for all neural network modules", "Loss function", "Optimizer", "Data loader"], "correctAnswer": 0, "explanation": "nn.Module is the foundational class for building models in PyTorch."},
        {"question": "What is eager execution in TensorFlow?", "options": ["Immediate operation execution without building graphs", "Delayed execution", "Static graphs", "Compilation"], "correctAnswer": 0, "explanation": "Eager execution evaluates operations immediately, like PyTorch's default."},
        {"question": "What is model.save() for?", "options": ["Persisting trained models to disk", "Training models", "Deleting models", "Loading data"], "correctAnswer": 0, "explanation": "save() serializes model weights and architecture for later use."},
        {"question": "What is torch.optim?", "options": ["Module containing optimization algorithms", "Loss functions", "Activation functions", "Data loaders"], "correctAnswer": 0, "explanation": "torch.optim provides optimizers like SGD, Adam, AdamW."},
        {"question": "What is TensorBoard?", "options": ["Visualization toolkit for tracking experiments", "Database", "Cloud service", "Programming language"], "correctAnswer": 0, "explanation": "TensorBoard visualizes metrics, graphs, and model architectures."},
        {"question": "What is torch.cuda.is_available()?", "options": ["Checks if GPU is available for training", "Loads data", "Saves model", "Compiles code"], "correctAnswer": 0, "explanation": "This function detects CUDA-enabled GPUs for acceleration."},
        {"question": "What is model.parameters()?", "options": ["Returns iterator over model parameters for optimization", "Data loader", "Loss function", "Activation"], "correctAnswer": 0, "explanation": "parameters() accesses trainable weights for optimizers."},
        {"question": "What is torch.no_grad()?", "options": ["Context manager disabling gradient computation for inference", "Training mode", "Data loader", "Loss function"], "correctAnswer": 0, "explanation": "no_grad() reduces memory and speeds up inference by skipping gradients."},
        {"question": "What is Dataset and DataLoader pattern?", "options": ["Dataset stores data; DataLoader provides batching and shuffling", "Loss function", "Optimizer", "Model architecture"], "correctAnswer": 0, "explanation": "This pattern separates data storage from batching logic."},
        {"question": "What is model checkpointing?", "options": ["Saving model at intervals during training", "Final save only", "Data backup", "Code versioning"], "correctAnswer": 0, "explanation": "Checkpointing enables recovery and tracks best models during training."}
      ]
    },
    {
      "id": "advanced-architectures",
      "title": "Advanced Architectures: CNNs & Transformers (4-6 weeks)",
      "description": "Master Computer Vision with CNNs, object detection (YOLO), and NLP with Transformers, BERT, GPT. Learn RNN/LSTM for sequential data.",
      "resources": [
        { "title": "Stanford CS231n - CNNs", "url": "https://www.youtube.com/embed/videoseries?si=bD5dbaC9gUID3k45&amp;list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv", "type": "Video"},
        { "title": "CNN Explained - DeepLizard", "url": "https://www.youtube.com/embed/YRhxdVk_sIs?si=GocVmRzur8a-2L6l", "type": "Video" },
        { "title": "How CNNs Work", "url": "https://www.youtube.com/embed/FmpDIaiMIeA?si=-XcCtiZnWqIpr0yt", "type": "Video" },
        { "title": "ResNet Paper Explained", "url": "https://www.youtube.com/embed/GWt6Fu05voI?si=LjjWkxRdNgZ3tEHw", "type": "Video" },
        { "title": "Transfer Learning - PyTorch", "url": "https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html", "type": "Article" },
        { "title": "Image Classification PyTorch", "url": "https://www.youtube.com/embed/videoseries?si=xiARCYXmswDo0PQB&amp;list=PL3Dh_99BJkCEhE7Ri8W6aijiEqm3ZoGRq", "type": "Video" },
        { "title": "YOLO Object Detection", "url": "https://www.youtube.com/embed/ag3DLKsl2vk?si=D5pxf2Ir7ZOayWc5", "type": "Video" },
        { "title": "YOLOv8 Complete Guide", "url": "https://docs.ultralytics.com/", "type": "Article" },
        { "title": "Object Detection PyTorch", "url": "https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html", "type": "Article" },
        { "title": "Stanford CS224n - NLP", "url": "http://web.stanford.edu/class/cs224n/", "type": "Course", "note": "BEST NLP Course" },
        { "title": "Illustrated Transformer", "url": "https://jalammar.github.io/illustrated-transformer/", "type": "Article", "note": "BEST Visual Guide" },
        { "title": "Attention Is All You Need", "url": "https://www.youtube.com/embed/iDulhoQ2pro?si=Ne0CpqQhN01HE81C" , "type": "Video" },
        { "title": "Transformers from Scratch", "url": "https://www.youtube.com/embed/U0s0f995w14?si=_yaD1iVS6ImRGmPj", "type": "Video" },
        { "title": "BERT Explained", "url": "https://jalammar.github.io/illustrated-bert/", "type": "Article" },
        { "title": "GPT Explained", "url": "https://jalammar.github.io/illustrated-gpt2/", "type": "Article" },
        { "title": "Hugging Face Transformers", "url": "https://www.youtube.com/embed/videoseries?si=t884bokXcbaYtXOU&amp;list=PLo2EIpI_JMQvWfQndUesu0nPBAtZ9gP1o", "type": "Course", "note": "FREE" },
        { "title": "Fine-tuning BERT Part01", "url": "https://www.youtube.com/embed/x66kkDnbzi4?si=3HHe2Xk9GMvQev_D", "type": "Video" },
        { "title": "Fine-tuning BERT Part02", "url": "https://www.youtube.com/embed/Hnvb9b7a_Ps?si=pQ-amrZNlBNdtdoe" , "type": "Video" },
        { "title": "RNN Explained", "url": "https://www.youtube.com/embed/AsNTP8Kwu80?si=dBfurT5xI0xax4h-" , "type": "Video" },
        { "title": "LSTM Networks", "url": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/", "type": "Article", "note": "Best Explanation" }
      ],
      "quiz": [
        {"question": "What is a CNN (Convolutional Neural Network)?", "options": ["Neural network designed for processing grid-like data (images)", "Database", "Web server", "Text processor"], "correctAnswer": 0, "explanation": "CNNs use convolutional layers to automatically learn spatial hierarchies."},
        {"question": "What is a convolutional layer?", "options": ["Layer applying filters to detect features like edges and textures", "Fully connected layer", "Output layer", "Input layer"], "correctAnswer": 0, "explanation": "Convolutional layers slide filters across images to extract local patterns."},
        {"question": "What is pooling in CNNs?", "options": ["Database operation", "Downsampling operation reducing spatial dimensions", "Activation function", "Loss function"], "correctAnswer": 1, "explanation": "Pooling (max/average) reduces dimensionality while retaining important features."},
        {"question": "What is YOLO?", "options": ["You Only Look Once - real-time object detection algorithm", "Database", "Programming language", "Cloud service"], "correctAnswer": 0, "explanation": "YOLO detects objects in a single forward pass, enabling real-time performance."},
        {"question": "What is a Transformer?", "options": ["Architecture using self-attention for sequence modeling", "Image classifier", "Database", "Web framework"], "correctAnswer": 0, "explanation": "Transformers revolutionized NLP by processing sequences in parallel with attention."},
        {"question": "What is self-attention?", "options": ["Mechanism weighing importance of different sequence positions", "Loss function", "Optimizer", "Data augmentation"], "correctAnswer": 0, "explanation": "Self-attention enables models to focus on relevant parts of the input."},
        {"question": "What is BERT?", "options": ["Database", "Bidirectional Encoder Representations from Transformers", "Image classifier", "Cloud service"], "correctAnswer": 1, "explanation": "BERT pre-trains on masked language modeling for contextualized embeddings."},
        {"question": "What is GPT?", "options": ["Generative Pre-trained Transformer for text generation", "Image model", "Database", "Web server"], "correctAnswer": 0, "explanation": "GPT is an autoregressive model trained to predict next tokens."},
        {"question": "What is ResNet?", "options": ["Residual Network using skip connections for very deep networks", "Database", "Optimizer", "Loss function"], "correctAnswer": 0, "explanation": "ResNet's skip connections solve vanishing gradients, enabling 100+ layer networks."},
        {"question": "What is an LSTM?", "options": ["Long Short-Term Memory - RNN variant handling long sequences", "Database", "Activation function", "Image model"], "correctAnswer": 0, "explanation": "LSTMs use gates to control information flow, capturing long-term dependencies."},
        {"question": "What is attention in Transformers?", "options": ["Query-key-value mechanism computing relevance", "Pooling", "Convolution", "Normalization"], "correctAnswer": 0, "explanation": "Attention computes weighted sums based on query-key similarity."},
        {"question": "What is positional encoding?", "options": ["Adding position information to token embeddings", "Data normalization", "Weight initialization", "Loss function"], "correctAnswer": 0, "explanation": "Positional encodings inform Transformers about token order."},
        {"question": "What is multi-head attention?", "options": ["Multiple parallel attention mechanisms", "Single attention", "Pooling layer", "Convolution"], "correctAnswer": 0, "explanation": "Multi-head attention captures different relationship types simultaneously."},
        {"question": "What is VGG architecture?", "options": ["Deep CNN with small 3x3 filters", "Transformer", "RNN", "GAN"], "correctAnswer": 0, "explanation": "VGG uses stacked small filters for deep feature hierarchies."},
        {"question": "What is Inception module?", "options": ["Parallel convolutions of different sizes", "Single convolution", "Pooling only", "Fully connected"], "correctAnswer": 0, "explanation": "Inception captures multi-scale features in parallel."},
        {"question": "What is encoder-decoder architecture?", "options": ["Encoder processes input; decoder generates output", "Single encoder", "Single decoder", "No connection"], "correctAnswer": 0, "explanation": "Used in seq2seq tasks like translation."},
        {"question": "What is RNN (Recurrent Neural Network)?", "options": ["Network with loops for sequential data", "Feedforward network", "CNN", "GAN"], "correctAnswer": 0, "explanation": "RNNs process sequences by maintaining hidden states."},
        {"question": "What is GRU?", "options": ["Gated Recurrent Unit - simplified LSTM variant", "CNN", "Transformer", "GAN"], "correctAnswer": 0, "explanation": "GRU uses reset and update gates, fewer parameters than LSTM."},
        {"question": "What is pre-training?", "options": ["Training on large dataset before fine-tuning", "Final training", "Data preprocessing", "Model deployment"], "correctAnswer": 0, "explanation": "Pre-training learns general features for transfer learning."},
        {"question": "What is masked language modeling?", "options": ["Predicting masked tokens in text", "Image masking", "Data hiding", "Encryption"], "correctAnswer": 0, "explanation": "MLM is BERT's pre-training objective for bidirectional context."}
      ]
    },
    {
      "id": "mlops-deployment",
      "title": "Production ML & MLOps (3-4 weeks)",
      "description": "Learn experiment tracking (MLflow, W&B), model versioning, deployment with FastAPI, Docker, and monitoring best practices.",
      "resources": [
        { "title": "Made With ML", "url": "https://madewithml.com/", "type": "Article" },
        { "title": "MLflow Documentation", "url": "https://mlflow.org/docs/latest/index.html", "type": "Article" },
        { "title": "Weights & Biases Tutorials", "url": "https://www.youtube.com/embed/videoseries?si=M6Cd01ZpxQkqIpFY&amp;list=PLD80i8An1OEGajeVo15ohAQYF1Ttle0lk" , "type": "Video" },
        { "title": "DVC Documentation", "url": "https://www.youtube.com/embed/XMnuZF53LAY?si=Yy3xvL5nHUstOGFt", "type": "Video" },
        { "title": "FastAPI Documentation", "url": "https://fastapi.tiangolo.com/", "type": "Article" },
        { "title": "TorchServe", "url": "https://pytorch.org/serve/", "type": "Article" },
        { "title": "Full Stack Deep Learning", "url": "https://www.youtube.com/embed/videoseries?si=uyhKifobrsCBILIz&amp;list=PL1T8fO7ArWlcWg04OgNiJy91PywMKT2lv", "type": "Course" },
        { "title": "Docker for Data Science", "url": "https://docker-curriculum.com/", "type": "Article" }
      ],
      "quiz": [
        {"question": "What is MLOps?", "options": ["Practices for deploying and maintaining ML models in production", "Database", "Programming language", "Cloud provider"], "correctAnswer": 0, "explanation": "MLOps combines ML, DevOps, and data engineering for reliable model lifecycle."},
        {"question": "What is MLflow?", "options": ["Open-source platform for ML experiment tracking and model management", "Database", "Web framework", "Cloud service"], "correctAnswer": 0, "explanation": "MLflow tracks experiments, packages models, and manages deployment."},
        {"question": "What is Weights & Biases (W&B)?", "options": ["Database", "Platform for experiment tracking and visualization", "Cloud storage", "Programming language"], "correctAnswer": 1, "explanation": "W&B logs metrics, hyperparameters, and visualizations for ML experiments."},
        {"question": "What is model versioning?", "options": ["Tracking different model iterations and configurations", "Data cleaning", "Feature engineering", "Loss function"], "correctAnswer": 0, "explanation": "Versioning enables reproducibility and rollback of model deployments."},
        {"question": "What is FastAPI?", "options": ["Modern web framework for building APIs with Python", "Database", "ML library", "Cloud service"], "correctAnswer": 0, "explanation": "FastAPI is used to deploy ML models as REST APIs with automatic documentation."},
        {"question": "What is Docker?", "options": ["Platform for containerizing applications for consistent deployment", "Database", "Programming language", "Cloud provider"], "correctAnswer": 0, "explanation": "Docker packages models with dependencies for portable, reproducible deployments."},
        {"question": "What is model monitoring?", "options": ["Tracking model performance and data drift in production", "Training models", "Data collection", "Feature engineering"], "correctAnswer": 0, "explanation": "Monitoring detects performance degradation and triggers retraining."},
        {"question": "What is data drift?", "options": ["Change in input data distribution over time", "Model accuracy", "Training speed", "Memory usage"], "correctAnswer": 0, "explanation": "Data drift causes model performance to degrade as production data shifts."},
        {"question": "What is A/B testing in ML?", "options": ["Comparing model performance between two versions on real users", "Data cleaning", "Feature selection", "Hyperparameter tuning"], "correctAnswer": 0, "explanation": "A/B testing validates new models before full deployment."},
        {"question": "What is CI/CD for ML?", "options": ["Continuous Integration/Deployment automating model training and deployment", "Database operation", "Manual deployment", "Data storage"], "correctAnswer": 0, "explanation": "CI/CD pipelines automate testing, versioning, and deployment of ML models."},
        {"question": "What is model serving?", "options": ["Database operation", "Deploying models for real-time or batch inference", "Model training", "Data preprocessing"], "correctAnswer": 1, "explanation": "Model serving provides APIs and infrastructure for production predictions."},
        {"question": "What is TorchServe?", "options": ["Framework for serving PyTorch models at scale", "Training library", "Database", "Cloud service"], "correctAnswer": 0, "explanation": "TorchServe simplifies deployment with multi-model serving and auto-scaling."},
        {"question": "What is model quantization?", "options": ["Reducing precision of model weights for faster inference", "Data preprocessing", "Feature engineering", "Model architecture"], "correctAnswer": 0, "explanation": "Quantization converts float32 to int8, reducing model size and latency."},
        {"question": "What is model pruning?", "options": ["Removing unimportant weights to compress models", "Data cleaning", "Feature selection", "Hyperparameter tuning"], "correctAnswer": 0, "explanation": "Pruning eliminates low-magnitude weights for efficiency."},
        {"question": "What is SHAP?", "options": ["SHapley Additive exPlanations for model interpretability", "Database", "Optimizer", "Loss function"], "correctAnswer": 0, "explanation": "SHAP assigns feature importance values for individual predictions."},
        {"question": "What is model explainability?", "options": ["Understanding model decisions and predictions", "Model training", "Data collection", "Feature engineering"], "correctAnswer": 0, "explanation": "Explainability builds trust and meets regulatory requirements like GDPR."},
        {"question": "What is canary deployment?", "options": ["Gradual rollout to subset of users before full deployment", "Complete replacement", "Training strategy", "Data migration"], "correctAnswer": 0, "explanation": "Canary deployments minimize risk by testing on small traffic percentage."},
        {"question": "What is shadow deployment?", "options": ["Running new model alongside old without affecting production", "Replacing old model", "Training mode", "Data backup"], "correctAnswer": 0, "explanation": "Shadow mode validates new models by comparing predictions offline."},
        {"question": "What is blue-green deployment?", "options": ["Two identical environments for instant switching and rollback", "Single environment", "Training strategy", "Data split"], "correctAnswer": 0, "explanation": "Blue-green enables zero-downtime updates with instant rollback capability."},
        {"question": "What is model governance?", "options": ["Policies for ethical, compliant, and responsible AI deployment", "Model training", "Data governance only", "Code review"], "correctAnswer": 0, "explanation": "Governance ensures fairness, transparency, and accountability in ML systems."}
      ]
    },
    {
      "id": "mlops-pipeline",
      "title": "MLOps Pipeline (3-4 weeks)",
      "description": "Build CI/CD for ML, automated training pipelines, model monitoring, data drift detection, and infrastructure as code.",
      "resources": [
        { "title": "MLOps Course", "url": "https://www.youtube.com/embed/w71RHxAWxaM?si=PRQOZcy9XXQyoqrX", "type": "Course" },
        { "title": "Designing ML Systems", "url": "https://www.youtube.com/embed/videoseries?si=gjwH9YLar115Cl2q&amp;list=PLLm69KFEX6JD4fJ7ge8-WEDw3G4c_nzHs", "type": "Book Explanation" }
      ],
      "quiz": [
        {"question": "What is an ML pipeline?", "options": ["Automated workflow for data processing, training, and deployment", "Database", "Web server", "File system"], "correctAnswer": 0, "explanation": "ML pipelines orchestrate end-to-end workflows from data to deployed models."},
        {"question": "What is feature store?", "options": ["Database", "Centralized repository for storing and serving ML features", "Model registry", "Cloud storage"], "correctAnswer": 1, "explanation": "Feature stores ensure consistent features across training and inference."},
        {"question": "What is model registry?", "options": ["Centralized repository for versioned ML models", "Database", "Web framework", "Cloud service"], "correctAnswer": 0, "explanation": "Model registries track model lineage, versions, and metadata."},
        {"question": "What is Kubeflow?", "options": ["ML toolkit for deploying workflows on Kubernetes", "Database", "Programming language", "Web server"], "correctAnswer": 0, "explanation": "Kubeflow provides scalable ML pipelines on Kubernetes infrastructure."},
        {"question": "What is DVC (Data Version Control)?", "options": ["Tool for versioning datasets and ML experiments", "Database", "Cloud provider", "Web framework"], "correctAnswer": 0, "explanation": "DVC tracks data and model versions like Git tracks code."},
        {"question": "What is infrastructure as code (IaC) in MLOps?", "options": ["Managing ML infrastructure through code (Terraform, CloudFormation)", "Manual setup", "Database query", "Model training"], "correctAnswer": 0, "explanation": "IaC automates reproducible infrastructure provisioning for ML systems."},
        {"question": "What is shadow deployment?", "options": ["Running new model alongside old without affecting users", "Deleting models", "Training models", "Data collection"], "correctAnswer": 0, "explanation": "Shadow mode compares new model predictions with production before switching."},
        {"question": "What is canary deployment?", "options": ["Database migration", "Gradually rolling out new model to subset of users", "Complete replacement", "Model training"], "correctAnswer": 1, "explanation": "Canary releases minimize risk by testing on small user percentage first."},
        {"question": "What is model retraining?", "options": ["Periodically updating models with new data", "Deleting models", "Initial training", "Model evaluation"], "correctAnswer": 0, "explanation": "Regular retraining keeps models accurate as data distributions change."},
        {"question": "What is feature drift?", "options": ["Change in feature statistics over time", "Model accuracy", "Training speed", "Data storage"], "correctAnswer": 0, "explanation": "Feature drift indicates distribution shifts requiring model updates."},
        {"question": "What is a DAG in ML pipelines?", "options": ["Database", "Directed Acyclic Graph representing task dependencies", "Model architecture", "Loss function"], "correctAnswer": 1, "explanation": "DAGs define workflow execution order in orchestration tools like Airflow."},
        {"question": "What is experiment tracking?", "options": ["Recording hyperparameters, metrics, and artifacts for reproducibility", "Model deployment", "Data collection", "Feature engineering"], "correctAnswer": 0, "explanation": "Tracking enables comparison and reproduction of ML experiments."},
        {"question": "What is hyperparameter tuning?", "options": ["Optimizing model configuration parameters", "Feature engineering", "Data preprocessing", "Model deployment"], "correctAnswer": 0, "explanation": "Tuning (grid search, Bayesian) finds optimal hyperparameters."},
        {"question": "What is AutoML?", "options": ["Automated machine learning for model selection and optimization", "Manual ML", "Data collection", "Deployment only"], "correctAnswer": 0, "explanation": "AutoML automates architecture search and hyperparameter tuning."},
        {"question": "What is model selection?", "options": ["Choosing best model based on validation metrics", "Random selection", "First model", "Largest model"], "correctAnswer": 0, "explanation": "Selection balances accuracy, latency, and resource constraints."},
        {"question": "What is ensemble serving?", "options": ["Deploying multiple models for combined predictions", "Single model", "Data serving", "Feature serving"], "correctAnswer": 0, "explanation": "Ensembles aggregate predictions for improved accuracy and robustness."},
        {"question": "What is rollback strategy?", "options": ["Database backup", "Reverting to previous model version on failure", "Forward only", "Deleting models"], "correctAnswer": 1, "explanation": "Rollback minimizes downtime when new deployments fail."},
        {"question": "What is performance budgeting?", "options": ["Setting latency/throughput targets for production models", "Cost budgeting", "Data budget", "Time budget"], "correctAnswer": 0, "explanation": "Performance budgets ensure models meet SLA requirements."},
        {"question": "What is metadata management?", "options": ["Tracking data lineage, schemas, and quality metrics", "Model training", "Deployment", "Code review"], "correctAnswer": 0, "explanation": "Metadata enables governance, debugging, and reproducibility."},
        {"question": "What is Airflow?", "options": ["Workflow orchestration platform for data pipelines", "ML framework", "Database", "Cloud service"], "correctAnswer": 0, "explanation": "Airflow schedules and monitors complex data and ML workflows."}
      ]
    },
    {
      "id": "large-language-models",
      "title": "Large Language Models (LLMs) (4-6 weeks)",
      "description": "Master LLM fundamentals, Transformers, prompt engineering, fine-tuning (LoRA, QLoRA), RAG, LangChain, and vector databases.",
      "resources": [
        { "title": "Intro of LLM", "url": "https://www.youtube.com/embed/RBzXsQHjptQ?si=wj0IrYuWKl8qPP2O", "type": "Video", "duration": "5 hours" },
        { "title": "Let's Build GPT", "url": "https://www.youtube.com/embed/kCc8FmEb1nY?si=ZXx7KGqCD93VdunZ", "type": "Video", "note": "Andrej Karpathy - BEST" },
        { "title": "State of GPT", "url": "https://www.youtube.com/embed/bZQun8Y4L2A?si=_OGJVRwXOmvM5AbD", "type": "Video", "note": "Andrej Karpathy - Must Watch" },
        { "title": "How GPT Works", "url": "https://www.youtube.com/embed/wjZofJX0v4M?si=nsWzzSebPpKrFWg9", "type": "Video" },
        { "title": "Prompt Engineering Guide", "url": "https://www.promptingguide.ai/", "type": "Article", "note": "Complete" },
        { "title": "ChatGPT Prompt Engineering", "url": "https://www.youtube.com/embed/_ZvnD73m40o?si=QKs8pum-tutKuoBw" , "type": "Course"},
        { "title": "LoRA & QLoRA Fine-tuning Explained", "url": "https://www.youtube.com/embed/t1caDsMzWBk?si=iKLu-ql9ShUNBipY", "type": "Video" },
        { "title": "RAG Explained - LangChain", "url": "https://www.youtube.com/embed/wd7TZ4w1mSw?si=Cw8jbF8vm2Ndvv92", "type": "Video" },
        { "title": "RAG from Scratch", "url": "https://www.youtube.com/embed/sVcwVQRHIc8?si=uHQgKXPgEPFyHy4-", "type": "Video" },
        { "title": "LangChain Full Course", "url": "https://www.youtube.com/embed/LbT1yp6quS8?si=P2kKVxXtOohKO1vQ", "type": "Video", "duration": "2 hours" },
        { "title": "LangChain Documentation", "url": "https://python.langchain.com/docs/get_started/introduction", "type": "Article" },
        { "title": "LangChain Crash Course", "url": "https://www.youtube.com/embed/_v_fgW2SkkQ?si=9bLZT3cV0B7yr3hm", "type": "Video" },
        { "title": "Vector Databases Explained", "url": "https://www.youtube.com/embed/dN0lsF2cvm4?si=NMTG2oo4qjqRgs0O", "type": "Video" },
        { "title": "Pinecone Tutorial", "url": "https://www.youtube.com/embed/videoseries?si=YUMk__7CP-WkSqsg&amp;list=PLRLVhGQeJDTLiw-ZJpgUtZW-bseS2gq9-", "type": "Video" }
      ],
      "quiz": [
        {"question": "What is a Large Language Model (LLM)?", "options": ["Neural network trained on vast text data for language understanding", "Database", "Web server", "Image classifier"], "correctAnswer": 0, "explanation": "LLMs like GPT-4 generate human-like text and understand context."},
        {"question": "What is prompt engineering?", "options": ["Crafting inputs to elicit desired outputs from LLMs", "Model training", "Data cleaning", "Feature engineering"], "correctAnswer": 0, "explanation": "Effective prompts guide LLMs through instructions, examples, and context."},
        {"question": "What is fine-tuning?", "options": ["Database operation", "Adapting pre-trained model to specific task with new data", "Data cleaning", "Model deployment"], "correctAnswer": 1, "explanation": "Fine-tuning specializes general models for domain-specific applications."},
        {"question": "What is LoRA (Low-Rank Adaptation)?", "options": ["Efficient fine-tuning method using low-rank matrices", "Database", "Cloud service", "Web framework"], "correctAnswer": 0, "explanation": "LoRA reduces parameters to fine-tune, making it memory-efficient."},
        {"question": "What is RAG (Retrieval-Augmented Generation)?", "options": ["Combining retrieval and generation for knowledge-grounded responses", "Database", "Optimizer", "Loss function"], "correctAnswer": 0, "explanation": "RAG retrieves relevant documents and uses them to generate accurate answers."},
        {"question": "What is LangChain?", "options": ["Framework for building LLM-powered applications", "Database", "Cloud provider", "Image model"], "correctAnswer": 0, "explanation": "LangChain chains LLM calls with memory, tools, and data sources."},
        {"question": "What is a vector database?", "options": ["Database storing and querying high-dimensional embeddings", "Relational database", "File system", "Web server"], "correctAnswer": 0, "explanation": "Vector databases enable semantic search using embedding similarity."},
        {"question": "What are embeddings?", "options": ["Dense vector representations capturing semantic meaning", "Images", "Databases", "Web pages"], "correctAnswer": 0, "explanation": "Embeddings map text to vectors where similar meanings are close together."},
        {"question": "What is zero-shot learning?", "options": ["Model performing tasks without task-specific training", "Supervised learning", "Reinforcement learning", "Transfer learning"], "correctAnswer": 0, "explanation": "Zero-shot uses pre-trained knowledge to handle new tasks via prompts."},
        {"question": "What is few-shot learning?", "options": ["Learning from small number of examples in the prompt", "Large dataset training", "Unsupervised learning", "Data augmentation"], "correctAnswer": 0, "explanation": "Few-shot provides examples in the prompt to guide model behavior."},
        {"question": "What is tokenization in LLMs?", "options": ["Database indexing", "Splitting text into subword units for model processing", "Data encryption", "Model training"], "correctAnswer": 1, "explanation": "Tokenization converts text to tokens (BPE, WordPiece) for transformer input."},
        {"question": "What is BPE (Byte Pair Encoding)?", "options": ["Tokenization algorithm merging frequent character pairs", "Encryption method", "Database protocol", "Web standard"], "correctAnswer": 0, "explanation": "BPE creates vocabulary by iteratively merging common byte pairs."},
        {"question": "What is context window in LLMs?", "options": ["Maximum number of tokens model can process at once", "Window function", "Database query", "Browser window"], "correctAnswer": 0, "explanation": "Context window limits input length (e.g., 4K, 8K, 128K tokens)."},
        {"question": "What is attention masking?", "options": ["Preventing model from attending to certain positions", "Image masking", "Data hiding", "Security feature"], "correctAnswer": 0, "explanation": "Masking controls which tokens can attend to each other (causal, bidirectional)."},
        {"question": "What is PEFT (Parameter-Efficient Fine-Tuning)?", "options": ["Fine-tuning methods updating only small subset of parameters", "Full fine-tuning", "Pre-training", "Deployment"], "correctAnswer": 0, "explanation": "PEFT includes LoRA, adapters, prefix tuning for efficient adaptation."},
        {"question": "What is instruction tuning?", "options": ["Fine-tuning on instruction-following datasets", "Pre-training", "Data cleaning", "Model deployment"], "correctAnswer": 0, "explanation": "Instruction tuning teaches models to follow user instructions effectively."},
        {"question": "What is RLHF (Reinforcement Learning from Human Feedback)?", "options": ["Training models using human preference rankings", "Supervised learning", "Pre-training", "Data collection"], "correctAnswer": 0, "explanation": "RLHF aligns models with human values through reward modeling."},
        {"question": "What is chain-of-thought prompting?", "options": ["Encouraging models to show reasoning steps", "Direct answering", "Data preprocessing", "Model architecture"], "correctAnswer": 0, "explanation": "Chain-of-thought improves reasoning by making intermediate steps explicit."},
        {"question": "What is temperature in LLM sampling?", "options": ["Parameter controlling randomness in text generation", "Model training rate", "Hardware temperature", "Data quality metric"], "correctAnswer": 0, "explanation": "Higher temperature increases creativity; lower increases determinism."},
        {"question": "What is perplexity in language models?", "options": ["Metric measuring model uncertainty on text", "Database metric", "Training loss", "Accuracy score"], "correctAnswer": 0, "explanation": "Lower perplexity indicates better model fit to the data."}
      ]
    },
    {
      "id": "computer-vision-advanced",
      "title": "Computer Vision Advanced (3-4 weeks, Optional)",
      "description": "Explore Vision Transformers, Diffusion models (Stable Diffusion), GANs, multi-modal models (CLIP), 3D vision.",
      "resources": [
        { "title": "PyImageSearch", "url": "https://pyimagesearch.com/", "type": "Article" },
         { "title": "Diffusion Models", "url": "https://www.youtube.com/embed/i2qSxMVeVLI?si=rUmfT7IXtUfe1fM9", "type": "Video" },
        { "title": "Diffusion Models Tutorial", "url": "https://lilianweng.github.io/posts/2021-07-11-diffusion-models/", "type": "Article" },
        { "title": "CLIP by OpenAI", "url": "https://openai.com/research/clip", "type": "Article" }
      ],
      "quiz": [
        {"question": "What is a Vision Transformer (ViT)?", "options": ["Transformer architecture adapted for image classification", "CNN variant", "Database", "Web framework"], "correctAnswer": 0, "explanation": "ViT splits images into patches and processes them as sequences."},
        {"question": "What are Diffusion Models?", "options": ["Database", "Generative models adding/removing noise iteratively", "Classification models", "Optimization algorithms"], "correctAnswer": 1, "explanation": "Diffusion models like Stable Diffusion generate images by denoising."},
        {"question": "What is Stable Diffusion?", "options": ["Text-to-image generation model using diffusion process", "Database", "Web framework", "Cloud service"], "correctAnswer": 0, "explanation": "Stable Diffusion generates realistic images from text prompts."},
        {"question": "What is a GAN (Generative Adversarial Network)?", "options": ["Two networks (generator and discriminator) competing", "Classification model", "Database", "Optimizer"], "correctAnswer": 0, "explanation": "GANs train a generator to fool a discriminator, creating realistic data."},
        {"question": "What is CLIP?", "options": ["Contrastive Language-Image Pre-training for vision-language tasks", "Database", "Cloud service", "Web server"], "correctAnswer": 0, "explanation": "CLIP learns joint embeddings for images and text for zero-shot classification."},
        {"question": "What is image segmentation?", "options": ["Partitioning image into meaningful regions or objects", "Image classification", "Object detection", "Image generation"], "correctAnswer": 0, "explanation": "Segmentation assigns class labels to each pixel (semantic/instance)."},
        {"question": "What is object detection?", "options": ["Database query", "Identifying and localizing objects with bounding boxes", "Image classification", "Segmentation"], "correctAnswer": 1, "explanation": "Object detection finds where objects are and what they are (YOLO, Faster R-CNN)."},
        {"question": "What is instance segmentation?", "options": ["Distinguishing individual object instances with pixel masks", "Semantic segmentation", "Classification", "Detection"], "correctAnswer": 0, "explanation": "Instance segmentation separates different instances of the same class."},
        {"question": "What is data augmentation in computer vision?", "options": ["Artificially expanding dataset with transformations", "Data cleaning", "Feature engineering", "Model training"], "correctAnswer": 0, "explanation": "Augmentation applies rotations, flips, crops to increase training diversity."},
        {"question": "What is attention mechanism in vision?", "options": ["Focusing on important image regions dynamically", "Loss function", "Optimizer", "Data loader"], "correctAnswer": 0, "explanation": "Attention weighs feature importance, used in Transformers and attention-based CNNs."},
        {"question": "What is transfer learning in computer vision?", "options": ["Database transfer", "Using pre-trained models for new vision tasks", "Training from scratch", "Data migration"], "correctAnswer": 1, "explanation": "Transfer learning leverages ImageNet-trained models for custom tasks."},
        {"question": "What is fine-tuning in CV?", "options": ["Adapting pre-trained layers to new task", "Training from scratch", "Data preprocessing", "Model deployment"], "correctAnswer": 0, "explanation": "Fine-tuning updates some/all layers on task-specific data for better performance."},
        {"question": "What is self-supervised learning in CV?", "options": ["Learning from unlabeled data using pretext tasks", "Supervised learning", "Manual labeling", "Data collection"], "correctAnswer": 0, "explanation": "Self-supervision learns representations without manual annotations (rotation, colorization)."},
        {"question": "What is contrastive learning?", "options": ["Learning by comparing similar and dissimilar samples", "Supervised learning", "Data augmentation", "Loss function only"], "correctAnswer": 0, "explanation": "Contrastive methods (SimCLR, MoCo) pull similar samples together, push dissimilar apart."},
        {"question": "What is DINO?", "options": ["Self-DIstillation with NO labels for vision transformers", "CNN architecture", "Database", "Optimizer"], "correctAnswer": 0, "explanation": "DINO trains ViTs without labels using knowledge distillation."},
        {"question": "What is MAE (Masked Autoencoder)?", "options": ["Self-supervised method masking and reconstructing image patches", "Supervised learning", "GAN variant", "Object detection"], "correctAnswer": 0, "explanation": "MAE learns representations by predicting masked patches, like BERT for images."},
        {"question": "What is object tracking?", "options": ["Following objects across video frames", "Object detection", "Image classification", "Segmentation"], "correctAnswer": 0, "explanation": "Tracking maintains object identities over time using algorithms like SORT, DeepSORT."},
        {"question": "What is 3D vision?", "options": ["Understanding spatial structure and depth from images", "2D classification", "Image enhancement", "Data augmentation"], "correctAnswer": 0, "explanation": "3D vision includes depth estimation, point clouds, and 3D reconstruction."},
        {"question": "What is NeRF (Neural Radiance Fields)?", "options": ["Database", "Neural representation for 3D scene synthesis from 2D views", "2D classifier", "Optimizer"], "correctAnswer": 1, "explanation": "NeRF generates novel photorealistic views from multi-view images."},
        {"question": "What is semantic segmentation?", "options": ["Classifying each pixel into categories", "Object detection", "Image classification", "Instance segmentation"], "correctAnswer": 0, "explanation": "Semantic segmentation labels pixels by class without instance differentiation."}
      ]
    },
    {
      "id": "reinforcement-learning",
      "title": "Reinforcement Learning (4-6 weeks, Optional)",
      "description": "Learn Markov Decision Processes, Q-Learning, Deep Q-Networks, Policy Gradients, Actor-Critic, PPO, multi-agent RL.",
      "resources": [
        { "title": "Spinning Up in Deep RL", "url": "https://spinningup.openai.com/", "type": "Article", "note": "OpenAI" },
        { "title": "Reinforcement Learning", "url": "https://www.youtube.com/embed/Mut_u40Sqz4?si=rrL9FpEk6WrcTAEm", "type": "Course" },
        { "title": "Deep RL Book", "url": "https://www.youtube.com/embed/wDVteayWWvU?si=p1qoyNOm4KwIy_pq", "type": "Video", "note": "Overview" }
      ],
      "quiz": [
        {"question": "What is Reinforcement Learning?", "options": ["Learning through interaction with environment via rewards", "Supervised learning", "Unsupervised learning", "Transfer learning"], "correctAnswer": 0, "explanation": "RL agents learn optimal actions by maximizing cumulative rewards."},
        {"question": "What is a Markov Decision Process (MDP)?", "options": ["Mathematical framework for RL with states, actions, rewards", "Database", "Neural network", "Optimization algorithm"], "correctAnswer": 0, "explanation": "MDPs model sequential decision-making where future depends only on current state."},
        {"question": "What is Q-Learning?", "options": ["Database query", "Value-based RL algorithm learning action-value function", "Supervised learning", "Classification"], "correctAnswer": 1, "explanation": "Q-Learning learns Q(s,a) - expected return from taking action a in state s."},
        {"question": "What is Deep Q-Network (DQN)?", "options": ["Neural network approximating Q-values for complex environments", "Database", "Web framework", "Cloud service"], "correctAnswer": 0, "explanation": "DQN uses deep learning to handle high-dimensional state spaces."},
        {"question": "What is policy gradient?", "options": ["RL method directly optimizing policy using gradients", "Loss function", "Data preprocessing", "Model evaluation"], "correctAnswer": 0, "explanation": "Policy gradients update policy parameters to maximize expected reward."},
        {"question": "What is PPO (Proximal Policy Optimization)?", "options": ["Database", "Policy gradient algorithm with clipped objective", "Supervised learning", "Classification method"], "correctAnswer": 1, "explanation": "PPO balances performance and stability in policy updates."},
        {"question": "What is the exploration-exploitation tradeoff?", "options": ["Balance between trying new actions and using known good actions", "Model training", "Data cleaning", "Feature engineering"], "correctAnswer": 0, "explanation": "Agents must explore to discover rewards while exploiting known strategies."},
        {"question": "What is Actor-Critic?", "options": ["RL architecture with separate policy (actor) and value (critic) networks", "Database", "Web server", "Cloud service"], "correctAnswer": 0, "explanation": "Actor proposes actions; critic evaluates them to guide learning."},
        {"question": "What is experience replay?", "options": ["Storing and resampling past experiences for training", "Real-time learning", "Data collection", "Model deployment"], "correctAnswer": 0, "explanation": "Replay breaks correlation between consecutive samples, improving stability."},
        {"question": "What is reward shaping?", "options": ["Designing intermediate rewards to guide learning", "Final reward only", "Data preprocessing", "Model architecture"], "correctAnswer": 0, "explanation": "Reward shaping provides feedback to accelerate learning of complex tasks."},
        {"question": "What is value iteration?", "options": ["Dynamic programming algorithm computing optimal value function", "Database iteration", "Loop structure", "Data processing"], "correctAnswer": 0, "explanation": "Value iteration iteratively updates value estimates until convergence."},
        {"question": "What is policy iteration?", "options": ["Algorithm alternating between policy evaluation and improvement", "Database operation", "Model training", "Data preprocessing"], "correctAnswer": 0, "explanation": "Policy iteration finds optimal policy through iterative refinement."},
        {"question": "What is temporal difference (TD) learning?", "options": ["Learning from bootstrapped estimates of future rewards", "Time series analysis", "Database operation", "Feature engineering"], "correctAnswer": 0, "explanation": "TD methods update estimates using current predictions of future values."},
        {"question": "What is SARSA?", "options": ["On-policy TD algorithm (State-Action-Reward-State-Action)", "Database protocol", "Web framework", "Cloud service"], "correctAnswer": 0, "explanation": "SARSA learns Q-values following the current policy."},
        {"question": "What is A3C (Asynchronous Advantage Actor-Critic)?", "options": ["Parallel RL algorithm with multiple agents", "Database", "Web server", "Compiler"], "correctAnswer": 0, "explanation": "A3C trains multiple agents asynchronously for faster learning."},
        {"question": "What is DDPG (Deep Deterministic Policy Gradient)?", "options": ["Actor-critic for continuous action spaces", "Database", "Discrete action method", "Supervised learning"], "correctAnswer": 0, "explanation": "DDPG extends DQN to continuous control tasks."},
        {"question": "What is SAC (Soft Actor-Critic)?", "options": ["Off-policy algorithm maximizing entropy for exploration", "Database", "Image classifier", "Web framework"], "correctAnswer": 0, "explanation": "SAC balances reward maximization with policy entropy."},
        {"question": "What is multi-agent RL?", "options": ["Multiple agents learning simultaneously in shared environment", "Single agent", "Database operation", "Web service"], "correctAnswer": 0, "explanation": "Multi-agent RL handles cooperation, competition, and communication."},
        {"question": "What is curriculum learning in RL?", "options": ["Training on progressively harder tasks", "Random task order", "Single task only", "Data preprocessing"], "correctAnswer": 0, "explanation": "Curriculum learning accelerates learning through structured progression."},
        {"question": "What is reward engineering?", "options": ["Designing reward functions to guide desired behavior", "Data engineering", "Feature engineering", "Model architecture"], "correctAnswer": 0, "explanation": "Well-designed rewards prevent unintended behaviors and improve learning."}
      ]
    }
  ],
  "projects": [
    {
      "title": "House Price Prediction Model",
      "description": "Build a regression model to predict house prices using features like location, size, and amenities. Great introduction to supervised learning.",
      "difficulty": "Beginner",
      "duration": "2-3 weeks",
      "skills": ["Python", "Scikit-learn", "Pandas", "Data Preprocessing", "Linear Regression"],
      "tasks": [
        "Load and explore housing dataset",
        "Handle missing values and outliers",
        "Perform feature engineering",
        "Split data into train and test sets",
        "Train linear regression and random forest models",
        "Evaluate model performance with RMSE and RÂ²",
        "Visualize predictions vs actual values",
        "Create simple web interface with Streamlit"
      ]
    },
    {
      "title": "Image Classification with CNNs",
      "description": "Develop a convolutional neural network to classify images from CIFAR-10 or custom dataset using TensorFlow/Keras.",
      "difficulty": "Intermediate",
      "duration": "3-4 weeks",
      "skills": ["Deep Learning", "CNN", "TensorFlow", "Keras", "Image Processing"],
      "tasks": [
        "Understand CNN architecture (Conv, Pool, FC layers)",
        "Load and preprocess image dataset",
        "Implement data augmentation",
        "Build CNN model from scratch",
        "Apply transfer learning with ResNet/VGG",
        "Train model with callbacks and early stopping",
        "Evaluate with confusion matrix and accuracy",
        "Visualize feature maps and filters",
        "Deploy model with TensorFlow Serving"
      ]
    },
    {
      "title": "Natural Language Processing Sentiment Analyzer",
      "description": "Create an NLP system for sentiment analysis on product reviews or social media using transformers and BERT.",
      "difficulty": "Intermediate",
      "duration": "4-5 weeks",
      "skills": ["NLP", "Transformers", "BERT", "Hugging Face", "Text Processing"],
      "tasks": [
        "Collect and preprocess text data",
        "Tokenize text using BERT tokenizer",
        "Fine-tune pre-trained BERT model",
        "Implement attention visualization",
        "Train on sentiment classification task",
        "Evaluate with precision, recall, F1-score",
        "Handle class imbalance",
        "Build REST API for predictions",
        "Create interactive demo app"
      ]
    },
    {
      "title": "Recommendation System",
      "description": "Build a sophisticated recommendation engine using collaborative filtering, content-based filtering, and hybrid approaches with neural networks.",
      "difficulty": "Advanced",
      "duration": "5-6 weeks",
      "skills": ["Recommendation Systems", "Matrix Factorization", "Deep Learning", "Feature Engineering", "Evaluation Metrics"],
      "tasks": [
        "Study collaborative and content-based filtering",
        "Implement user-based and item-based CF",
        "Build matrix factorization with SVD",
        "Develop neural collaborative filtering",
        "Create hybrid recommendation approach",
        "Handle cold start problem",
        "Implement online learning for real-time updates",
        "Evaluate with MAP, NDCG, and coverage",
        "Add A/B testing framework",
        "Deploy scalable system with caching"
      ]
    },
    {
      "title": "Autonomous AI Agent with Reinforcement Learning",
      "description": "Develop an autonomous agent that learns to navigate complex environments using deep reinforcement learning (DQN, PPO, A3C).",
      "difficulty": "Advanced",
      "duration": "7-9 weeks",
      "skills": ["Reinforcement Learning", "DQN", "PPO", "OpenAI Gym", "Neural Networks"],
      "tasks": [
        "Set up OpenAI Gym environment",
        "Implement Q-Learning and Deep Q-Network",
        "Add experience replay and target networks",
        "Develop policy gradient methods (REINFORCE)",
        "Implement Proximal Policy Optimization",
        "Add curiosity-driven exploration",
        "Train agent on complex tasks",
        "Visualize learning curves and policies",
        "Compare different RL algorithms",
        "Deploy agent in custom environment",
        "Create video demonstrations"
      ]
    }
  ]
}